set_engine("ranger",importance="impurity")%>%
fit(mu~., data=df_recipe %>% prep() %>% juice())
vip(vip.imp.model)
vip.imp.model=rand_forest(trees = 1000)%>%
set_mode("regression")%>%
set_engine("ranger",importance="impurity")%>%
fit(mu~., data=df_recipe %>% prep() %>% juice())
df_recipe
df_recipe=recipe(mu~., data=train.data)%>%
update_role(order,new_role = "nodes")%>%
step_zv(all_predictors())%>%
step_normalize(all_predictors())%>%
step_corr(all_predictors(), threshold = 0.7, method = "spearman")
df_recipe
df_recipe
vip.imp.model=rand_forest(trees = 1000)%>%
set_mode("regression")%>%
set_engine("ranger",importance="impurity")%>%
fit(mu~., data= df_recipe %>% prep() %>% juice())
vip.imp.model=rand_forest(trees = 1000)%>%
set_mode("regression")%>%
set_engine("ranger",importance="impurity")%>%
randomForest::fit(mu~., data= df_recipe %>% prep() %>% juice())
vip.imp.model=rand_forest(trees = 1000)%>%
set_mode("regression")%>%
set_engine("ranger",importance="impurity")%>%
base::fit(mu~., data= df_recipe %>% prep() %>% juice())
library(here)
library(recipeselectors)
install.packages(recipeselectors)
install.packages('recipeselectors')
devtools::install_github("stevenpawley/recipeselectors")
library(recipeselectors)
devtools::install_github("stevenpawley/colino")
library(colino)
library(recipeselectors)
library(here)
vip.imp.model=rand_forest(trees = 1000)%>%
set_mode("regression")%>%
set_engine("ranger",importance="impurity")%>%
fit(mu~., data= df_recipe %>% prep() %>% juice())
?fit
vip.imp.model <- rand_forest(trees = 500) %>%
set_mode("regression") %>%
set_engine("ranger", importance = "impurity") %>%
fit(mu~., data = df_recipe%>% prep() %>% juice())
vip(vip.imp.model)
vi(vip.imp.model) %>%
filter(Importance > 0)
df.rec=recipe(mu~., data=train.data)%>%
step_dummy(all_nominal(one_hot=TRUE))%>%
prep() %>% juice ()
df.rec=recipe(mu~., data=train.data)%>%
step_dummy(all_nominal(),one_hot=TRUE)%>%
prep() %>% juice ()
df.rec
vip.imp.model <- rand_forest(trees = 500) %>%
set_mode("regression") %>%
set_engine("ranger", importance = "impurity") %>%
fit(mu~., data = df_prep%>% prep() %>% juice())
vip.imp.model <- rand_forest(trees = 500) %>%
set_mode("regression") %>%
set_engine("ranger", importance = "impurity") %>%
fit(mu~., data = df.rec%>% prep() %>% juice())
vip.imp.model <- rand_forest(trees = 500) %>%
set_mode("regression") %>%
set_engine("ranger", importance = "impurity") %>%
parsnip::fit(mu~., data = df_recipe%>% prep() %>% juice())
vip(vip.imp.model)
vi(vip.imp.model) %>%
filter(Importance > 0)
vip::vi(vip.imp.model) %>%
filter(Importance > 0)
rfe.model <- rand_forest(mode = "regression") %>% set_engine("ranger", importance = "permutation")
rfe_rec <- recipe(mu~., data = train.data) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
step_medianimpute(all_numeric(), -all_outcomes()) %>%
step_select_vip(all_predictors(), outcome = "mu", model = rfe_model, threshold = 0.9)
rfe_rec %>%
prep() %>%
juice()
rfe.model <- rand_forest(mode = "regression") %>% set_engine("ranger", importance = "permutation")
rfe_rec <- recipe(mu~., data = train.data) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
step_impute_median(all_numeric(), -all_outcomes()) %>%
step_select_vip(all_predictors(), outcome = "mu", model = rfe.model, threshold = 0.9)
rfe_rec %>%
prep() %>%
juice()
library(recipes)
library(parsnip)
rfe_rec <- recipe(mu~., data = train.data) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
step_impute_median(all_numeric(), -all_outcomes()) %>%
step_select_vip(all_predictors(), outcome = "mu", model = rfe.model, threshold = 0.9)
rfe_rec <- recipe(mu~., data = train.data) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
step_impute_median(all_numeric(), -all_outcomes()) %>%
step_select(all_predictors(), outcome = "mu", model = rfe.model, threshold = 0.9)
rfe_rec %>%
prep() %>%
juice()
rfe_rec <- recipe(mu~., data = train.data) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
step_impute_median(all_numeric(), -all_outcomes()) %>%
parsnip::step_select_vip(all_predictors(), outcome = "mu", model = rfe.model, threshold = 0.9)
rfe_rec %>%
prep() %>%
juice()
rfe.model
rfe_rec <- recipe(mu~., data = train.data) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
step_impute_median(all_numeric(), -all_outcomes())
rfe_rec
rfe_rec  %>%
parsnip::step_select_vip(all_predictors(), outcome = "mu", model = rfe.model, threshold = 0.9)
library(vip)
rfe_rec  %>%
parsnip::step_select_vip(all_predictors(), outcome = "mu", model = rfe.model, threshold = 0.9)
rfe_rec <- recipe(mu~., data = train.data) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
step_impute_median(all_numeric(), -all_outcomes()) %>%
vip::step_select_vip(all_predictors(), outcome = "mu", model = rfe.model, threshold = 0.9)
library(recipes)
library(parsnip)
##--Method 4: recursive feature elimination
rfe.model <- rand_forest(mode = "regression") %>%
set_engine("ranger", importance = "permutation")
rfe_rec <- recipe(mu~., data = train.data) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
step_impute_median(all_numeric(), -all_outcomes()) %>%
step_select_vip(all_predictors(), outcome = "mu", model = rfe.model,
top_p = 10, threshold = 0.9)
step_select_boruta
installed.packages("colino")
library("colino")
devtools::install_github("stevenpawley/colino")
library(recipes)
install.packages("installr")
library(installr)
updateR()
devtools::install_github("stevenpawley/colino")
updateR()
library(installr)
updateR()
library(devtools)
Rcpp::sourceCpp("C:/Users/rcappaw/Desktop/R/Workflow/igraphEpi-New/c++functions.cpp")
library(devtools)
find_rtools()
install.packages("Rtools")
install.packages("sp")
library(devtools)
find_rtools()
write('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', file = "~/.Renviron", append = TRUE)
Sys.which("make")
install.packages("jsonlite", type = "source")
find_rtools()
library(devtools)
find_rtools()
find_rtools()
library(devtools)
find_rtools()
setwd("C:/Users/rcappaw/Desktop/R/Workflow/igraphEpi-New")
suppressPackageStartupMessages({
library(Matrix)
library(stats)
library(igraph)
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(janitor)
library(vip)
})
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+ Data generation for the machine learning model
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#++++++++++++++++++++++++++++++++
#+ Helper functions
#+++++++++++++++++++++++++++++++
##########################################################################################################
# Input an igraph object from a file, treating it as a static graph
##########################################################################################################
getGraphFromFile <- function(file, simplify=TRUE, useBiggestComponent=TRUE, asUndirected=TRUE) {
dat <- read.table(file) # just read static graph: ignoring third column
G <- graph_from_data_frame(dat)
if (asUndirected==TRUE) {
G <- as.undirected(G, "collapse")
}
#  g_names <- gsub(".edges","",networks[i]) # one edge for each pair of connect vertices (not sure what this is for)
if (useBiggestComponent==TRUE) {
netclust <- clusters(G) #look for subgraphs
gcc <- V(G)[netclust$membership == which.max(netclust$csize)]#select vertices from the largest sub-graph
G <- induced.subgraph(G, gcc) #make it a igraph object.
}
if (simplify==TRUE) {
G <- igraph::simplify(G, remove.multiple = TRUE, remove.loops = TRUE)
}
return(G)
}
###--Normalized Laplacina function--##
normalized_laplacian=function(Graphs){
laplacian_matrix(Graphs,normalized = T)
}
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+ Calculate graph features
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++
calcGraphFeatures <- function(Graphs=NULL) {
features <- c(
"order",                    # number of vertices
"edges",                     # number of edges
"connected",                # True / False
"max_component",            # maximum component size (=order iff the graph is connected)
"minDegree",                # minimum degree of any vertex
"maxDegree",                # maximum degree of any vertex
"mean_degree",                # average degree of any vertex
"minCut",                   # minimum cut weight of the graph (might take a while to compute)
"FiedlerValue",             # second-highest eigenvalue of the Laplacian matrix
"Normalized_FiedlerValue",   # second-highest eigenvalue of the Normaized Laplacian matrix
"closeness_centr",                # average inverse of distance between any pair of vertices
"modularity",               # DEFINITION REQUIRED
"diameter",                 # maximum distance between any two vertices (NAN if not connected)
"betw_centr",              # max_{v} proportion of shortest paths going through vertex v
"transitivity",             # aka Clustering Coefficient, is proportion of connected triples that form triangles: e.g., (a--b--c--a) when (a--b--c) is present.
"threshold",                 # 1/max(eigen value of A)
"spectral_radius"         # max (eigen value of A)
)
df <- as.data.frame(matrix(ncol=length(features),nrow=length(Graphs)))
colnames(df)=features
# Stuff that is simple to apply and needs no interim components:
df$order = base::as.numeric(lapply(Graphs, gorder))
df$edges = base::as.numeric(lapply(Graphs, gsize))
df$connected = base::as.numeric(lapply(Graphs, is_connected))
df$minCut = base::as.numeric(lapply(Graphs, min_cut))
df$diameter = base::as.numeric(lapply(Graphs, diameter))
df$transitivity = base::as.numeric(lapply(Graphs, transitivity))
# stuff that needs interim things:
degrees = lapply(Graphs,igraph::degree )
df$minDegree = base::as.numeric(lapply(degrees, min))
df$maxDegree = base::as.numeric(lapply(degrees, max))
df$mean_degree = base::as.numeric(lapply(degrees, mean))
# stuff that lapply doesn't like so has to be done in a loop:
communities <-lapply(Graphs, cluster_walktrap) #lapply(Graphs, cluster_leading_eigen)
Adj <- lapply(Graphs, as_adjacency_matrix)
L <- lapply(Graphs, laplacian_matrix)
Norm_Lap<-lapply(Graphs, normalized_laplacian)
Fiedler.value=NULL
norm.fiedler.value=NULL
for (i in 1:length(Graphs)) {
if (is.null(Graphs[[i]]$type)) { Graphs[[i]]$type = "untyped" }
df$modularity[i] <- modularity(communities[[i]])
df$spectral_radius[i] <- eigen(Adj[[i]], symmetric=TRUE, only.values=TRUE)$values[1]
Fiedler.value[[i]]=eigen(L[[i]], symmetric=TRUE, only.values=TRUE)$values
df$FiedlerValue[i] <- Fiedler.value[[i]][length(Fiedler.value[[i]])-1]
norm.fiedler.value[[i]]=eigen(Norm_Lap[[i]], symmetric=TRUE, only.values=TRUE)$values
df$Normalized_FiedlerValue[i] <- norm.fiedler.value[[i]][length(norm.fiedler.value[[i]])-1]
df$eigen_centr[i] <- centr_eigen(Graphs[[i]])$centralization
df$deg_centr[i] <- centr_degree(Graphs[[i]])$centralization
df$betw_centr[i] <- centr_betw(Graphs[[i]])$centralization
df$max_component[i] <- max(components(Graphs[[i]])$csize)
df$mean_eccentr[i]<-mean(eccentricity(Graphs[[i]]))
df$radius[i]<-radius(Graphs[[i]])
df$mean_path_length[i]<-average.path.length(Graphs[[i]])
#df$trace[i]<-sum(diag(Adj[[i]]))
df$graph_energy[i]<-sum(abs(eigen(Adj[[i]], symmetric=TRUE, only.values=TRUE)$values))
df$min_triangle[i]= min(count_triangles(Graphs[[i]]))
df$mean_triangle[i]= mean(count_triangles(Graphs[[i]]))
df$sd_triangle[i]= sd(count_triangles(Graphs[[i]]))
df$max_triangle[i]= max(count_triangles(Graphs[[i]]))
df$num_triangle[i]= sum(count_triangles(Graphs[[i]]))
df$deg_assort_coef[i]=assortativity_degree(Graphs[[i]])
df$threshold[i] <- 1/(df$spectral_radius[i])
if (df$connected[i]==TRUE) {
df$closeness_centr[i] = mean(closeness(Graphs[[i]]))
} else { # handle the case where G isn't connected
df$closeness_centr[i] = -1
}
}
return (df)
}
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+ Multiple Calculation of graph features
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++
## Used to perform runs on multiple simulated graphs on any given network
RunSimOnGraphFeatures<-function(Graphs, nreps=nreps,output_file=NULL, seed=-1) {
set.seed(1)
# ### Definition and initialization of parameters for graphfeatures
graphProperties=list()
# ### Definition and initialization of Graph Prefix
graphid=list();graphreplicate=list(); graphname=list(); GraphPrefix=list(); analysis=list()
for (g in 1:length(Graphs)){
for (reps in 1:nreps) {
### Calculate the graph features for each simulated graph of all the synthetic networks
print(paste("Calculating graph features on", Graphs[[g]]$name))
#graphProperties[[reps]] <- calcGraphFeatures(Graphs[g])
graphProperties[[reps]] <- calcGraphFeatures(Graphs[g])
}
graphname[[g]]=Graphs[[g]]$type
graphid[[g]]=Graphs[[g]]$id
graphreplicate[[g]]=c(1:nreps)
GraphPrefix=cbind(graphname[[g]],graphid[[g]],graphreplicate[[g]])
colnames(GraphPrefix)=c("GraphName","GraphID","GraphReplicate")
analysis[[g]]=as.data.frame(cbind(GraphPrefix,graphProperties[[reps]]))
row.names(analysis[[g]])=1:nreps
}
All_results=do.call(rbind,analysis)
# write.csv(All_results, file=output_file)
return( All_results)
}
df1=read.csv("df.lat.csv")%>%
filter(order<=1000)
df1=df1%>%dplyr::select(c(GraphName,order,edges,mean_eccentr,
radius,mean_path_length,graph_energy,min_triangle,mean_triangle,
sd_triangle,modularity,diameter,betw_centr,transitivity,threshold,
spectral_radius,eigen_centr,deg_centr,
mean_degree,minCut,FiedlerValue,Normalized_FiedlerValue,
closeness_centr),max_triangle,num_triangle,deg_assort_coef)%>%
mutate_if(is.character,factor)
df2=read.csv("df.sbm.csv")%>%
filter(order<=1000)
df2=df2%>%dplyr::select(c(GraphName,order,edges,mean_eccentr,
radius,mean_path_length,graph_energy,min_triangle,mean_triangle,
sd_triangle,modularity,diameter,betw_centr,transitivity,threshold,
spectral_radius,eigen_centr,deg_centr,
mean_degree,minCut,FiedlerValue,Normalized_FiedlerValue,
closeness_centr),max_triangle,num_triangle,deg_assort_coef)%>%
mutate_if(is.character,factor)
df3=rbind(read.csv("df.er.50.csv",sep = ",", header = T),
read.csv("df.er.100.csv",sep = ",", header = T),
read.csv("df.er.150.csv",sep = ",", header = T),
read.csv("df.er.200.csv",sep = ",", header = T))
df3=df3%>%dplyr::select(c(GraphName,order,edges,mean_eccentr,
radius,mean_path_length,graph_energy,min_triangle,mean_triangle,
sd_triangle,modularity,diameter,betw_centr,transitivity,threshold,
spectral_radius,eigen_centr,deg_centr,
mean_degree,minCut,FiedlerValue,Normalized_FiedlerValue,
closeness_centr),max_triangle,num_triangle,deg_assort_coef)%>%
mutate_if(is.character,factor)
df4=rbind(read.csv("df.SP.50.csv",sep = ",", header = T),
read.csv("df.SP.100.csv",sep = ",", header = T),
read.csv("df.SP.150.csv",sep = ",", header = T))
df4=df4%>%dplyr::select(c(GraphName,order,edges,mean_eccentr,
radius,mean_path_length,graph_energy,min_triangle,mean_triangle,
sd_triangle,modularity,diameter,betw_centr,transitivity,threshold,
spectral_radius,eigen_centr,deg_centr,
mean_degree,minCut,FiedlerValue,Normalized_FiedlerValue,
closeness_centr),max_triangle,num_triangle,deg_assort_coef)%>%
mutate_if(is.character,factor)
Data=rbind(df1,df2,df3,df4)
##--Shuffle--data
Data = Data[sample(1:nrow(Data)), ]##shuffle row indices and randomly re order
Data%>%na.omit()
# Change data frame to tibble (similar to data-frame)
df=as_tibble(Data)
df%>%count(GraphName,sort=T)
# Show first n row of the data
df%>%
slice_head(n=10)
###--------Load the janitor package to clean the data---------###
df<-df%>%
#mutate(GraphName=factor(GraphName,levels=c("ER","sbm","LAT")))%>%
clean_names()
df%>%
glimpse()
###----Data Partitioning----###
set.seed(384)
##sample and split
df_split=df[sample(1:nrow(df)), ]%>%
replace(is.na(.), 0)%>%
initial_split(prop = 0.7)
train=training(df_split)
test=testing(df_split)
###----Print number of training and testing set----###
cat("training set:", nrow(train), "\n",
"testing set :", nrow(test), "\n")
#Setting up model with tunning parameters
rf.mod <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
####++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++############
#----Random----Forest
####++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++############
cores <- parallel::detectCores()
cores
#Setting up model with tunning parameters
rf.mod <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
#Creating recipe object
rf.rec <-
recipe(graph_name ~ ., data = train)
#Creating workflow object
rf.workflow <-
workflow() %>%
add_model(rf.mod) %>%
add_recipe(rf.rec)
# show what will be tuned
rf.mod
extract_parameter_set_dials(rf.mod)
#replacing any Na's in train and test set of deg_assort_coef column with 0
train=train%>%
replace(is.na(.), 0)
#
test=train%>%
replace(is.na(.), 0)
# Create validation set
validatn.set <- validation_split(train,
strata = graph_name,
prop = 0.80)
rf.fold=vfold_cv(train)
rf.resample=bootstraps(train)
#Result
set.seed(345)
rf.result <-
rf.workflow%>%
tune_grid(rf.fold,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc)
)
#Setting up model with tunning parameters
rf.mod <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger")%>%#, num.threads = cores) %>%
set_mode("classification")
#Creating recipe object
rf.rec <-
recipe(graph_name ~ ., data = train)
rf.prep <- prep(rf.rec)
rf.juiced <- juice(rf.prep)
#Creating workflow object
rf.workflow <-
workflow() %>%
add_model(rf.mod) %>%
add_recipe(rf.rec)
# show what will be tuned
rf.mod
extract_parameter_set_dials(rf.mod)
#replacing any Na's in train and test set of deg_assort_coef column with 0
train=train%>%
replace(is.na(.), 0)
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+ Train hyperparameters
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
test=train%>%
replace(is.na(.), 0)
# Create validation set
validatn.set <- validation_split(train,
strata = graph_name,
prop = 0.80)
rf.fold=vfold_cv(train)
rf.resample=bootstraps(train)
library(doParallel)
doParallel::registerDoParallel()
set.seed(345)
rf.result <-
rf.workflow%>%
tune_grid(rf.fold,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc)
)
####++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++############
#----Random----Forest
####++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++############
cores <- parallel::detectCores()
cores
#Setting up model with tunning parameters
rf.model.spec <-
rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
#Creating recipe object
rf.rec <-
recipe(graph_name ~ ., data = train)
rf.prep <- prep(rf.rec)
rf.juiced <- juice(rf.prep)
#Creating workflow object
rf.workflow <-
workflow() %>%
add_model(rf.model.spec) %>%
add_recipe(rf.rec)
# show what will be tuned
rf.model.spec
extract_parameter_set_dials(rf.model.spec)
#replacing any Na's in train and test set of deg_assort_coef column with 0
train=train%>%
replace(is.na(.), 0)
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#+ Train hyperparameters
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
test=train%>%
replace(is.na(.), 0)
# Create validation set
validatn.set <- validation_split(train,
strata = graph_name,
prop = 0.80)
rf.fold=vfold_cv(train)
rf.resample=bootstraps(train)
set.seed(345)
rf.result <-
rf.workflow%>%
tune_grid(rf.fold,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc)
)
#Show best metric
rf.result %>%
show_best(metric = "roc_auc")
rf.result
